---
title: "Homework 4"
author: "Brian Schetzsle"
date: "3/11/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("R/Homework2.R", local = knitr::knit_global())
library(mvtnorm)
library(kableExtra)
library(microbenchmark)
library(latex2exp)
```

# Question 1

```{r}
X = rbeta(1000, 4, 2)
EX2 = mean(X^2)
VarX2 = var(X^2)
conf_int = c(EX2 - 1.96*sqrt(VarX2), EX2 + 1.96*sqrt(VarX2))
```

To approximate $E[X^2]$, where $X \sim \text{beta(4,2)}$, using Monte Carlo I first simulate 1000 realizations from the distribution of X, calculate the square of each realization, and finally take the mean of the squares. I compare this to the known value of $E[X^2]$:

$$
\begin{aligned}
E[X^2] &= Var(X) + E[X]^2 \\\\
&=\frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha+\beta+1)} + \left(\frac{\alpha}{\alpha+\beta}\right)^2 \\\\
&= \frac{8}{252} + \left(\frac{4}{6}\right)^2 \\\\
&\approx 0.4762
\end{aligned}
$$

The Monte Carlo estimate of $E[X^2]$ is `r EX2` (which is pretty close to the true value) with Monte Carlo error of `r sqrt(VarX2)`, yielding a 95\% confidence interval for the approximation of [`r conf_int[1]` , `r conf_int[2]`]. Interestingly, the support of $X^2$ is [0,1], so this confidence interval is nearly the whole support.

# Question 2

## a)

The transition probabilities for this Markov Chain, $p_{ij}$ for $i \neq j$, are the product of the probability of choosing state j from state i, $q_{ij}$, and then accepting that proposal, $a_{ij}$.

$$
\begin{aligned}
p_{ij} &= q_{ij}a_{ij} \\\\
&= q_{ij} \left( \frac{\pi_{j}q_{ji}}{\pi_j q_{ji} + \pi_i q_{ij}}  \right)
\end{aligned}
$$

## b)


To show that $\mathbf{\pi}$ is a stationary distribution we must show that $\mathbf{\pi}^T P  = \mathbf{\pi}^T$ where P is the matrix of transition probabilities. Note: $p_{ii}$ is the probability of staying in the same state, which either happens by having a different state proposed and rejected or having the current state proposed (it is accepted automatically). Thus, 

$$
\begin{aligned}
p_{ii} &= q_{ii} + \sum_{j \neq i}q_{ij}(1-a_{ij}) \\\\
&= \sum_j q_{ij} - \sum_{j \neq i}q_{ij}a_{ij} \\\\
&= 1 - \sum_{j \neq i}p_{ij}
\end{aligned}
$$
It is also important to note:

$$
\pi_i p_{ij} = \pi_i q_{ij} a_{ij} = \pi_iq_{ij} \left(\frac{\pi_jq_{ji}}{\pi_iq_{ij}+\pi_jq_{ji}} \right) = \pi_jq_{ji} \left(\frac{\pi_iq_{ij}}{\pi_iq_{ij}+\pi_jq_{ji}} \right) = \pi_jp_{ji}
$$

With these two properties, showing $\mathbf{\pi}$ is a stationary distribution is straightforward:

$$
\begin{aligned}
\{\pi^T P \}_i &= \sum_j \pi_j p_{ji} \\\\
&= \pi_i p_{ii} + \sum_{j \neq i} \pi_j p_{ji} \\\\
&= \pi_i(1-\sum_{j \neq i}p_{ij}) + \sum_{j \neq i} \pi_j p_{ji} \\\\
&= \pi_i - \sum_{j \neq i}\pi_i p_{ij} + \sum_{j \neq i} \pi_j p_{ji} \\\\
&= \pi_i - \sum_{j \neq i}\pi_j p_{ji} + \sum_{j \neq i} \pi_j p_{ji} \\\\
&= \pi_i
\end{aligned}
$$

Thus, $\pi^TP = \pi^T$ so $\pi$ is a stationary distribution of the Markov Chain.


# Question 3

## a)

Our update is generated by the formula

$$
\begin{aligned}
&x^{prop} = x^{cur}e^X \\\\
\implies \qquad &log(x^{prop}) = log(x^{cur}) + X \\\\
\implies \qquad &log(x^{prop}) \sim N(log(x^{cur}), \sigma^2) \\\\
\implies \qquad &f(x^{prop}) = \frac{1}{x^{prop}\sigma\sqrt{2\pi}}e^{-\frac{(log(x^{prop}) - log(x^{cur}))^2}{2\sigma^2}}
\end{aligned}
$$

## b)

$$
\begin{aligned}
\frac{f(x^{prop}|\alpha,\beta)*f(x^{cur}|x^{prop})}{f(x^{cur}|\alpha,\beta)*f(x^{prop}|x^{cur})} &= \left[\frac{\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}(x^{prop})^{\alpha-1}(1-x^{prop})^{\beta-1}}{\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}(x^{cur})^{\alpha-1}(1-x^{cur})^{\beta-1}}\right]\left[\frac{\frac{1}{x^{cur}\sigma \sqrt{2\pi}}}{\frac{1}{x^{prop}\sigma \sqrt{2\pi}}} \right] \\\\
&= \left[\frac{(x^{prop})^{\alpha-1}(1-x^{prop})^{\beta-1}}{(x^{cur})^{\alpha-1}(1-x^{cur})^{\beta-1}}\right] \left[\frac{x^{prop}}{x^{cur}}\right] \\\\
&= \left(\frac{x^{prop}}{x^{cur}}\right)^\alpha \left(\frac{1-x^{prop}}{1-x^{cur}}\right)^{\beta-1}
\end{aligned}
$$

## c)

```{r}
MH = function(Xcur, sigma, alpha=4, beta=2, iterations=100){
  result = matrix(0,nrow=iterations)
  result[1] = Xcur
  for(i in 2:iterations){
    Xprop = Xcur * exp(rnorm(1, 0,sigma))
    if(runif(1) < (Xprop/Xcur)^alpha * ((1-Xprop)/(1-Xcur))^(beta-1)){
      Xcur=Xprop
    }
    result[i] = Xcur
  }
  return(result)
}

MH_result = MH(1, 1, iterations=5000)
```

The above function obtains samples of X from $\text{Beta}(4,2)$. These samples are squared and the mean is taken to obtain an estimate of $E[X^2]$: `r mean(MH_result^2)`, compared to the true value $\approx 0.4762$, which is very close.


```{r echo=FALSE}
plot(MH_result^2, type="l", ylim=c(0,1), main=TeX(r'(Metropolis Hastings Samples of $X^2$)'), ylab=TeX(r'($X^2$)'), xlab="Iteration")

hist(MH_result^2, xlab=TeX(r'($X^2$)'), main=TeX(r'(Histogram of Metropolis Hastings Samples of $X^2$)'))
```

\qquad
\qquad


Github repository is located here:

https://github.com/bschetzsle/STATS230

